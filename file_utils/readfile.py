from pandas import read_csv
from pandas import DataFrame
from datetime import datetime, timedelta
from agraph_utils import GetQuerysFromAgent, GetConn
import json
import copy
import pickle

def read_file(type_=False, pattern=False):
    """
    read bio2rdf sample data file
    this file need 3 data sources:
    service_logs_bio2rdf.txt: only need the first three columns which are [ip, time, query]
                              time in this file is like 13/Jun/2014:02:17:08, and after reading this file into dataframe, the time information
                              will be translated into datetime format.
    query_type.txt: provide the query form information, for each line in this file represents the query type of correspoding line 
                    in service_logs_bio2rdf.txt. this information will be the forth column in the output, key: type
                    this file is generated by java code
    query_pattern.txt: provide the mainBGP information, for each BGP, begins with Linei(i represents the line in service_logs_bio2rdf.txt file)
                       and following that is a "<sep>" and the mainBGP. This information will be the fifth column in output, key: pattern.
    input:
        type_: whether add type info
        pattern: whether add pattern info
    output: a dataFrame with keys [ip, time, query, type, pattern]
    """
    lines = open("service_logs_bio2rdf.txt", 'r', encoding='utf-8').readlines()
    db = []
    for line in lines:
        line = line.strip().split("<sep>")[0:3]
        db.append(line)
        
    db = DataFrame(db, columns = ['ip', 'time', 'query'])
        
    # change time to more stardard format
    months = {'Jan': '1', 'Feb': '2', 'Mar': '3', 'Apr': '4', 'May': '5', 'Jun': '6',
     'Jul': '7', 'Aug': '8', 'Sep': '9', 'Oct': '10', 'Nov': '11', 'Dec': '12'}

    for i in range(len(db)):
        time = db.iloc[i]['time']
        day = int(time.split('/')[0])
        month = int(months[time.split('/')[1]])
        year = int(time.split('/')[2][0:4])
        hour = int(time.split(':')[1])
        mins = int(time.split(':')[2])
        secs = int(time.split(':')[3][0:3])
        db.iloc[i]['time'] = datetime(year, month, day, hour=hour, minute=mins, second=secs)
    
    if type_:
        # add query_type
        fo = open('query_type.txt', 'r', encoding='utf-8')
        lines = fo.readlines()
        fo.close()
        
        query_type = list(np.zeros(len(db)))
        for line in lines:
            if "<sep>" not in line:
                continue
            index = int(line.strip().split("<sep>")[0][4:])
            query_type[index] = line.strip().split("<sep>")[1]
            
        db['type'] = query_type

    if pattern:    
        # add query pattern 
        fo = open('query_pattern.txt', 'r', encoding='utf-8')
        lines = fo.readlines()
        fo.close()
        
        query_pattern = list(np.zeros(len(db)))
        flag = []
        for i, line in enumerate(lines):
            if "Line" in line and "<sep>" in line:
                flag.append(i)

        for i, f in enumerate(flag):
            query = ''
            index = int(lines[f].split("<sep>")[0][4:])
            query = query + lines[f].split("<sep>")[1]
            if i == len(flag) - 1:
                break
            fi = flag[i+1] # next position
            ind = flag[i] + 1
            while ind < fi:
                query = query + lines[ind]
                ind += 1
            # query = re.sub('\n', '~', query)
            # query = re.sub(r'\s+', ' ', query)
            query_pattern[index] = query
        
        db['pattern'] = query_pattern
    
    return db

def GetIPTimes(data, column='ip'):
    """
    calculate the number of querys and time_span for each ip.

    input: DataFrame with keys:
                ip: or the unique code for one agent. str.
                time: timedate, the time when the query is executed.
            column: str, name for ip column, sometimes "agent"
    
    output: DataFrame with keys:
                ip: the same as input
                times: int, the number of querys for one agent
                time_span: timedelta, max(time) - min(time) for one agent
    """
    ips = list(set(data[column].tolist()))
    ip_times = []
    for ip in ips:
        temp = []
        temp.append(ip)
        ip_query = data.loc[data[column]==ip]
        temp.append(len(ip_query))
        times = ip_query['time'].tolist()
        times.sort()
        temp.append(times[-1]-times[0])
        ip_times.append(temp)
    ip_times = DataFrame(ip_times, columns = [column, 'times', 'time_span'])
    return ip_times

def GetIP_mt1(data, column='ip'):
    """
    using the result from GetIPTimes, get a list with ip whose execution time more than 1
    
    input: the same as GetIPTimes

    output: a list of ip whose execution time more than 1
    """
    ip_times = GetIPTimes(data, column=column)
    ips = list(set(ip_times[column].tolist()))
    ips_mt1 = ip_times.loc[ip_times['times']>1][column].tolist()
    ips_mt1 = list(set(ips_mt1))
    return ips_mt1

def changeTime(df):
    """
    change the time from "2011-03-19T18:20:53+01:00" to datetime

    input:
        df, DataFrame, keys: ip/agent, time, query
    
    output:
        df with its time changed
    """
    for i in range(len(df)):
        time = df.iloc[i]['time']
        time = datetime(int(time[0:4]), int(time[5:7]), int(time[8:10]), int(time[11:13]), int(time[14:16]), int(time[17:19]))
        df.iloc[i]['time'] = time
    return df

def readLSQ_csv(file_name):
    """
    read LSQ csv file and change its time.
    this LSQ csv file generated by virtuoso endpoint with the output format is "csv"
    """
    df = read_csv(file_name, encoding = "ISO-8859-1")
    df = changeTime(df)
    return df

def readSessionInfo(name, dir_='session2'):
    """
    session_time and session_len all saved into 'name'_session_time.txt and 'name'_session_len.txt
    for each line in this file represent session time span and number of querys in a session.

    input:
        name: means the head of the file
    output:
        session_len: a list with session len (int)
        session_time: a list with timedelta
    """
    len_file = 'docs/%s/%s_session_len.txt' % (dir_, name)
    time_file = 'docs/%s/%s_session_time.txt' % (dir_, name)
    fo = open(len_file, 'r')
    lines = fo.readlines()
    session_len = [int(line.strip()) for line in lines]
    fo.close()
    fo = open(time_file, 'r')
    lines = fo.readlines()
    session_time = []
    for line in lines:
        h = int(line.strip().split(' ')[2].split(':')[0])
        m = int(line.strip().split(' ')[2].split(':')[1])
        s = int(line.strip().split(' ')[2].split(':')[2])
        session_time.append(timedelta(hours=h, minutes=m, seconds=s))
    fo.close()
    return session_len, session_time

def read_json(file_name):
    fo = open(file_name)
    valued = json.load(fo)
    fo.close()
    return valued

def read_list(file_name, sep=None):
    """
    read list. 
    if sep == 'None', then one item for each line.
    if sep == '<sep>', then read to a dict.
    """
    lines = open(file_name, 'r').readlines()
    if sep == None:
        res = [x.strip() for x in lines]
    else:
        res = {}
        for line in lines:
            if sep in line:
                line = line.strip().split('<sep>')
                key = line[0]
                if key not in res.keys():
                    res[key] = []
                if len(line) > 1:
                    for l in line[1:]:
                        if l not in res[key]:
                            res[key].append(l)
            else:
                line = line.strip()
                if line not in res.keys():
                    res[line] = []
    return res

def read_errorList(name):
    file_name = 'docs/error/%s_parse_error.txt' % name
    error_list = read_list(file_name)
    for i in range(len(error_list)):
        # -1 - 0 = -1
        # -1 - 1 = -2
        ith = -1 - i
        if '<' in error_list[ith]:
            break
        
    if i == 0:
        return error_list

    temp = error_list[ith+1:]
    error_list = error_list[:ith+1]
    for i in temp:
        ti = '<%s>' % i
        if ti not in error_list:
            error_list.append(ti)
            print(ti)
    return error_list
                


def readTerms(data_source):
    # get terms file
    terms_file = 'docs/queryid2term/%s_terms.txt' % data_source
    terms_tp_file = 'docs/queryid2term/%s_terms_tp.txt' % data_source
    terms_dict = read_list(terms_file, sep='<sep>')
    terms_tp_dict = read_list(terms_tp_file, sep='<sep>')

    # merge two dict
    for id, t in terms_tp_dict.items():
        if id not in terms_dict.keys():
            terms_dict[id] = t
        else:
            print('%s should not in both dict!' % id)
            
    return terms_dict

def readAgent2Query(name, dbpedia=False):
    if dbpedia:
        return readAgent2Query_dbpedia(name)
    lines = open('docs/agent2query/%s_agent2query.txt' % name, 'r').readlines()
    res = {}
    for line in lines:
        temp = []
        line = line.strip().split('<sep>')
        agent = line[0]
        for i in range(int((len(line)-1)/2)):
            query = line[i*2+1]
            time = datetime.fromisoformat(line[i*2+2])
            temp.append([query, time])
        temp = DataFrame(temp, columns=['query', 'time'])
        temp = temp.sort_values(by='time')
        res[agent] = temp
    return res

def readAgent2Query_dbpedia(name):
    lines = open('docs/agent2query/%s_agent2query.txt' % name, 'r').readlines()
    data = read_csv_dbpedia(f'/root/data/lsq/dbpedia_csv/{name}.csv')
    res = {}
    for line in lines:
        temp = []
        line = line.strip().split('<sep>')
        agent = line[0]
        for i in range(int((len(line)-1)/2)):
            query = line[i*2+1]
            # ['OrientFile'] + '_' + str(querys.iloc[qi]['idxInFile']
            # from ipdb import set_trace; set_trace()
            idxInFile = int(query.split('_')[-1][:-1])
            query_text = data.loc[data['idxInFile']==idxInFile]['query'].tolist()[0]
            time = datetime.fromisoformat(line[i*2+2])
            temp.append([query_text, time, idxInFile])
        temp = DataFrame(temp, columns=['query', 'time', 'idxInFile'])
        temp = temp.sort_values(by=['time', 'idxInFile'])
        res[agent] = temp
    return res    

def read_valuedSession(name, filter_users=False, dbpedia=False):
    if dbpedia:
        return read_valuedSession_dbpedia(name)
    # from ipdb import set_trace; set_trace()
    lines = open('docs/valuedSession2/%s_valuedSession.txt' % name, 'r').readlines()
    res = []
    # ---------------remove atypical users-------------------
    if filter_users:
        users = readAtypicalUsers(name)
    # -------------------------------------------------------
    for line in lines:
        line = line.strip().split('<sep>')
        temp = {}
        # ------------------remove users--------------
        if filter_users and line[0] in users:
            continue
        # --------------------------------------------
        temp['agent'] = line[0]
        session = []
        for i in range(int((len(line)-1)/2)):
            query = line[i*2+1]
            time = datetime.fromisoformat(line[i*2+2])
            session.append([query, time])
        # from ipdb import set_trace; set_trace()
        session = DataFrame(session, columns=['query', 'time'])
        session = session.sort_values(by='time') 
        temp['session'] = session
        res.append(temp)
    return res

def read_valuedSession_dbpedia(name):
    # from ipdb import set_trace; set_trace()
    lines = open('docs/valuedSession2/%s_valuedSession.txt' % name, 'r').readlines()
    data = read_csv_dbpedia(f'data/dbpedia/dbpedia_csv/{name}.csv')
    res = []
    count = 0
    for line in lines:
        # from ipdb import set_trace; set_trace()
        line = line.strip().split('<sep>')
        # session length >= 40
        if len(line) >= 81:
            count += 1
            continue
        temp = {}
        temp['agent'] = line[0]
        session = []
        for i in range(int((len(line)-1)/2)):
            time = datetime.fromisoformat(line[i*2+1])
            idxInFile = int(line[i*2+2])
            query = data.loc[data['idxInFile']==idxInFile]['query'].tolist()[0]
            session.append([query, time, idxInFile])
        # from ipdb import set_trace; set_trace()
        session = DataFrame(session, columns=['query', 'time', 'idxInFile'])
        session = session.sort_values(by=['time', 'idxInFile']) 
        temp['session'] = session
        res.append(temp)
    print(f'{name} filter out {count} sessions.')
    return res

def read_session(lines, dbpedia=None, freq=False):
    if dbpedia is not None:
        return read_session_dbpedia(lines, dbpedia, freq=freq)
    res = []
    for line in lines:
        line = line.strip().split('<sep>')
        temp = {}
        temp['agent'] = line[0]
        session = []
        for i in range(int((len(line)-1)/2)):
            query = line[i*2+1]
            time = datetime.fromisoformat(line[i*2+2])
            session.append([query, time])
        # from ipdb import set_trace; set_trace()
        session = DataFrame(session, columns=['query', 'time'])
        session = session.sort_values(by='time') 
        temp['session'] = session
        res.append(temp)
    return res

def read_session_dbpedia(lines, data, freq=False):
    res = []
    for line in lines:
        # from ipdb import set_trace; set_trace()
        line = line.strip().split('<sep>')
        temp = {}
        temp['agent'] = line[0]
        session = []
        for i in range(int((len(line)-1)/2)):
            if not freq:
                time = datetime.fromisoformat(line[i*2+1])
                idxInFile = int(line[i*2+2])
            else:
                time = datetime.fromisoformat(line[i*2+2])
                idxInFile = int(line[i*2+1].split('_')[-1][:-1])                
            query = data.loc[data['idxInFile']==idxInFile]['query'].tolist()[0]
            session.append([query, time, idxInFile])
        # from ipdb import set_trace; set_trace()
        session = DataFrame(session, columns=['query', 'time', 'IdxInFile'])
        session = session.sort_values(by=['time', 'IdxInFile']) 
        temp['session'] = session
        res.append(temp)
    return res

def read_loop_result(name):
    lines = open('docs/loop_evaluate/%s_loop.txt' % name, 'r').readlines()
    sess_loop = read_session(lines)
    lines = open('docs/loop_evaluate/%s_no_loop.txt' % name, 'r').readlines()
    sess_no_loop = read_session(lines)
    return sess_loop, sess_no_loop

def read_loop_result_fre(name):
    """
    filter out sessions with length more than 80 and loop pattern
    """
    lines = open('docs/valuedSession/%s_valuedSession.txt' % name, 'r').readlines()
    sess_no_loop = read_session(lines)
    lines = open('docs/valuedSession/%s_loop.txt' % name, 'r').readlines()
    sess_loop = read_session(lines)
    return sess_loop, sess_no_loop

def read_loop_fre_sess_count(name):
    lines = open('docs/valuedSession/%s_valuedSession.txt' % name, 'r').readlines()
    no_loop = len(lines)
    lines = open('docs/valuedSession/%s_loop.txt' % name, 'r').readlines()
    loop = len(lines)
    return loop, no_loop

def read_freq_file(name, dbpedia=None, freq=True):
    """
    filter agents before organizing them as sessions
    """
    freq_querys = open('docs/frequency/%s_freq_query.txt' % name, 'r').readlines()
    freq_querys = read_session(freq_querys, dbpedia=dbpedia, freq=True)
    return freq_querys

def read_freq_sess_count(name):
    """
    get the number of sessions filtered by freq.
    """
    return len(open('docs/frequency/%s_freq_query.txt' % name, 'r').readlines())

def read_pkl(file_name):
    infile = open(file_name,'rb')
    new_dict = pickle.load(infile)
    infile.close()
    return new_dict

def readAtypicalUsers(name):
    res = []
    lines = open('docs/remove_checkEndpoint.txt').readlines()
    for i, line in enumerate(lines):
        if f'DATASET:{name}' in line:
            break
    for line in lines[i+1:]:
        if 'DATASET' in line:
            break
        user = line.strip().split('<sep>')[0]
        res.append(user)
    return res

def statisticFilter(name):
    """
    collect number of users filtered by different constraints
    """
    # frequency test
    freq = open('docs/frequency/%s_freq.txt' % name, 'r').readlines()
    freq_count = len(freq)
    # parse error
    error = open('docs/loop/%s_loop_sequence_readable.txt' % name, 'r').readlines()
    error_count = 0
    # loop based on whole sequence
    loop1_count = 0
    for errori in error:
        if 'LESS THAN 1' in errori:
            error_count += 1
        if 'TYPE' in errori:
            loop1_count += 1
    # repetition
    rep = open('docs/repetition/%s_repetition.txt' % name, 'r').readlines()
    rep_count = len(rep)
    # valued Session
    lines = open('docs/valuedSession/%s_valuedSession.txt' % name, 'r').readlines()
    sess_no_loop = read_session(lines)
    lines = open('docs/valuedSession/%s_loop.txt' % name, 'r').readlines()
    sess_loop = read_session(lines)
    sess_loop_count = countUsers(sess_loop)
    sess_no_loop_count = countUsers(sess_no_loop)
    return freq_count, error_count, loop1_count, rep_count, sess_loop_count, sess_no_loop_count

def statisticFilterAll(data_source):
    res = []
    for i in data_source:
        freq_count, error_count, loop1_count, rep_count, sess_loop_count, sess_no_loop_count = statisticFilter(i)
        res.append([i, freq_count, error_count, loop1_count, rep_count, sess_loop_count, sess_no_loop_count])
    df = DataFrame(res, columns=['dataSet', 'FilterByFreq', 'FilterByError', 'FilterByPreLoopDect', 
                                'FilterByRep', 'FilterByLoopDectInSess', 'Left'])
    df['UserCount'] = df['FilterByFreq'] + df['FilterByError'] + df['FilterByPreLoopDect'] + \
                  df['FilterByRep'] + df['FilterByLoopDectInSess'] + df['Left']
    df.to_csv('docs/basic/FilterUser_dbpediaAdded.csv', index=False)
    return df

def statisticFilterQueries(name, dbpedia=None):
    """
    collect number of queries filtered by different constraints
    """
    def countQueries(sess):
        queries = []
        for sessi in sess:
            queries.extend(sessi['session']['query'].tolist())
        return len(queries), len(list(set(queries)))

    if dbpedia == None:
        conn = GetConn(name)
        data = None
    else:
        data = read_csv_dbpedia(f'data/DBpedia/csv/{name}.csv')
    res = {}

    # frequency test
    if dbpedia == None:
        freq = read_freq_file(name)
    else:
        freq = read_freq_file(name, dbpedia=data, freq=True)
    freq_count, freq_count_unique = countQueries(freq)
    freq_users = open('docs/frequency/%s_freq.txt' % name, 'r').readlines()
    freq_users_count = len(freq_users)
    res['#users_Freq'] = freq_users_count
    res['#queries_Freq'] = freq_count
    res['#uniqueQueries_Freq'] = freq_count_unique


    # parse error
    error = open('docs/loop/%s_loop_sequence_readable.txt' % name, 'r').readlines()
    error_count = 0
    error_queries = []
    # loop based on whole sequence
    loop1_count = 0
    loop1_queries = []

    for errori in error:
        if 'LESS THAN 1' in errori:
            error_count += 1
            agenti = errori.split('<sep>')[0]
            if dbpedia == None:
                queriesForAgent = GetQuerysFromAgent(conn, agenti)
            else:
                queriesForAgent = data.loc[data['user']==agenti]

            error_queries.extend(queriesForAgent['query'].tolist())
        if 'TYPE' in errori:
            loop1_count += 1

    res['#users_Error'] = error_count
    res['#queries_Error'] = len(error_queries)
    res['#uniqueQueries_Error'] = len(list(set(error_queries)))

    loop_query_fo = open('docs/loop/%s_loop_sequence_query.txt' % name, 'r').readlines()
    if dbpedia == None:
        loop_query = read_session(loop_query_fo)
    else:
        loop_query = read_session(loop_query_fo, dbpedia=data, freq=True)
    res['#users_Loop1'] = loop1_count
    res['#queries_Loop1'], res['#uniqueQueries_Loop1'] = countQueries(loop_query)

    # repetition
    rep = open('docs/repetition/%s_repetition.txt' % name, 'r').readlines()
    agents = [i.strip() for i in rep]
    rep_queries = []
    res['#users_Rep'] = len(rep)
    for agenti in agents:
        if dbpedia == None:
            queriesForAgent = GetQuerysFromAgent(conn, agenti)
        else:
            queriesForAgent = data.loc[data['user']==agenti]

        rep_queries.extend(queriesForAgent['query'].tolist())
    res['#queries_Rep'], res['#uniqueQueries_Rep'] = len(rep_queries), len(list(set(rep_queries)))


    # valued Session
    lines = open('docs/valuedSession2/%s_valuedSession.txt' % name, 'r').readlines()
    sess_no_loop = read_session(lines, dbpedia=data)
    lines = open('docs/valuedSession2/%s_loop.txt' % name, 'r').readlines()
    sess_loop = read_session(lines, dbpedia=data)
    res['#users_Loop2'] = countUsers(sess_loop)
    res['#queries_Loop2'], res['#uniqueQueries_Loop2'] = countQueries(sess_loop)
    res['#users_Organic'] = countUsers(sess_no_loop)
    res['#queries_Organic'], res['#uniqueQueries_Organic'] = countQueries(sess_no_loop)
    res['#sessions_Organic'] = len(sess_no_loop)
    return res

def countUsers(sess):
    users = []
    for i in sess:
        if i['agent'] not in users:
            users.append(i['agent'])
    return len(users)

def GetUserSource(name):
    """
    those users who cannot form session is not included.
    """
    agent = []
    # frequency test
    freq = open('docs/frequency/%s_freq.txt' % name, 'r').readlines()
    freq = [x.strip() for x in freq]
    agent.extend(freq)

    # parse error
    error = open('docs/loop/%s_loop_sequence_readable.txt' % name, 'r').readlines()
    for errori in error:
        if 'LESS THAN 1' in errori:
            agenti = errori.split('<sep>')[0]
            if agenti not in agent:
                agent.append(agenti)
            else:
                print(f'error user: {agenti}')
            
    # loop based on whole sequence
    lines = open('docs/loop/%s_loop_sequence_query.txt' % name, 'r')
    sess = read_session(lines)
    users = GetUsers(sess)
    for ui in users:
        if ui not in agent:
            agent.append(ui)
        else:
            print(f'loop on sequence: {ui}')
            
    # repetition
    rep = open('docs/repetition/%s_repetition.txt' % name, 'r').readlines()
    for ui in rep:
        ui = ui.strip()
        if ui not in agent:
            agent.append(ui)
        else:
            print(f'repetition: {ui}')


    # valued Session
    lines = open('docs/valuedSession/%s_valuedSession.txt' % name, 'r').readlines()
    sess_no_loop = read_session(lines)
    users = GetUsers(sess_no_loop)
    for ui in users:
        if ui not in agent:
            agent.append(ui)
        else:
            print(f'left: {ui}')
            
    lines = open('docs/valuedSession/%s_loop.txt' % name, 'r').readlines()
    sess_loop = read_session(lines)
    users = GetUsers(sess_loop)
    for ui in users:
        if ui not in agent:
            agent.append(ui)
        else:
            print(f'loop on session: {ui}')

    return agent

def GetUsers(sess):
    users = []
    for i in sess:
        if i['agent'] not in users:
            users.append(i['agent'])
    return users


def GetQuerys(namei):
    valuedSession = read_valuedSession(namei)
    queries = []
    for sessi in valuedSession:
        sess = sessi['session']
        queries.extend(sess['query'].tolist())
    queries = list(set(queries))

    return queries

def read_csv_dbpedia(file_name):
    df = read_csv(file_name)
    time_list = df['time'].tolist()
    sta_time = [datetime.fromisoformat(i) for i in time_list]
    df['time'] = sta_time

    return df